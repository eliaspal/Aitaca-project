{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **0. Import dataset**"
      ],
      "metadata": {
        "id": "uCdrE7SF32Nx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "153viguR2u5B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confirm the dataset has been imported correctly\n",
        "df.head()"
      ],
      "metadata": {
        "id": "NlkDIHuq3oSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Data Exploration**"
      ],
      "metadata": {
        "id": "ZP50mQL95mGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.1 Nulls**"
      ],
      "metadata": {
        "id": "flHPENBFJ9E4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "GKRnDy3a5uBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Imputing the NaN in 'finger_line_37_ring' using mean\n",
        "mask_nan = df[\"finger_line_37_ring\"].isna()\n",
        "mean_val = df[\"finger_line_37_ring\"].mean()\n",
        "df.loc[mask_nan, \"finger_line_37_ring\"] = mean_val"
      ],
      "metadata": {
        "id": "ql6VZV597btG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isna().sum())"
      ],
      "metadata": {
        "id": "2aSzAIz05LKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on that every column has one nan, it's probably due to an empty row\n",
        "empty_rows = df[df.isnull().all(axis=1)]\n",
        "print(empty_rows)"
      ],
      "metadata": {
        "id": "OIKwPh9z6qaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping row 176 from the dataframe\n",
        "df = df.drop(index=176)"
      ],
      "metadata": {
        "id": "Lf3ofWbb6vGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. **Distribution**"
      ],
      "metadata": {
        "id": "qddyG7nc8WQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "uIct767P561V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_columns = [col for col in df.select_dtypes(include=['float64'])]\n",
        "\n",
        "# Histogram\n",
        "df[numerical_columns].hist(bins=20, figsize=(12, 10))\n",
        "plt.suptitle('Histogram')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fA4TpWlU9AR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Right-skewed distributions** for most of the variables\n",
        "\n",
        "**Long tails** which might indicate anatomical variability\n",
        "\n",
        "Many finger_line columns have very similar distributions, which means they may carry redundant information. Also, based on the right-skewed we should have outliers. A great choice is to use robustscaler because we are going to use regression and we want to keep the outliers as they are real data."
      ],
      "metadata": {
        "id": "dzwrtrRn9T3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Outliers**"
      ],
      "metadata": {
        "id": "I-ZzGAzb8e9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_columns = [col for col in df.select_dtypes(include=['float64'])]\n",
        "\n",
        "# Calculate number of rows for subplots (2 plots per row)\n",
        "num_cols = len(numeric_columns)\n",
        "num_rows = (num_cols + 1) // 2\n",
        "\n",
        "# Create the subplots dynamically (2 columns per row)\n",
        "fig, axes = plt.subplots(num_rows, 2, figsize=(12, num_rows * 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot one boxplot per numeric column\n",
        "for i, column in enumerate(numeric_columns):\n",
        "    sns.boxplot(y=df[column], ax=axes[i])\n",
        "    axes[i].set_title(f'Distribution of {column}')\n",
        "\n",
        "# Remove any unused subplot axes\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].remove()\n",
        "\n",
        "# Adjust layout to avoid overlap\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aN01l8Ck9V4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again we see what we discussed in the distribution."
      ],
      "metadata": {
        "id": "43IUacaxCbT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.3. Collinearity**"
      ],
      "metadata": {
        "id": "42YeeJ_hKLHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a mask for the upper triangle because the other traingle is its simmetry. This way we have a better visualization\n",
        "correlation_matrix = df.corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "\n",
        "# Drawing the heatmap with the mask\n",
        "plt.figure(figsize = (11,9))\n",
        "sns.heatmap(correlation_matrix, mask=mask, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, cmap=\"coolwarm\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8yAkIDQPACsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The heatmap confirms high collinearity among the fingers lines features. This is expected since these are sequential measurements along the same finger. This collinearity can harm linear models but is well-handled by tree-based models. Therefore, we will proceed using Random Forest, which is robust to redundant features.\n",
        "\n",
        "We could analyze the multicollinerity but, since the heatmap already reveals extreme collinearity, it is pointless.\n"
      ],
      "metadata": {
        "id": "OYmpkZq3EB64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'file' column since it's just an identification feature\n",
        "df = df.drop(columns=[\"file\"])"
      ],
      "metadata": {
        "id": "pLJwfWV1FQbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Preprocessing**"
      ],
      "metadata": {
        "id": "0_ea2gGGFcng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected RobustScaler to normalize the input features due to the presence of outliers and skewed distributions as we talked before.\n",
        "\n",
        "Unlike StandardScaler, which uses the mean and standard deviation and is sensitive to outliers, RobustScaler uses the median and interquartile range (IQR), making it more resilient to extreme values.\n",
        "\n",
        "Also, we are applying PCA due to the high collinearity observed.\n",
        "\n",
        "This choices allows us to simplify the prepocessing, avoiding to handle manually outliers or make feature selection."
      ],
      "metadata": {
        "id": "Ls1Tz6EhGOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(columns=[\"ring_GT\"])\n",
        "y = df[\"ring_GT\"]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features using RobustScaler\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA to reduce the number of features\n",
        "pca = PCA(n_components=5)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)"
      ],
      "metadata": {
        "id": "udGJI7bKFiQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. **Model Selection & Training**"
      ],
      "metadata": {
        "id": "3zhChy3yHahk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected a Random Forest Regressor as our baseline model due to several reasons:\n",
        "\n",
        "* It handles high-dimensional input and multicollinearity well, which is essential given the redundancy of the features.\n",
        "* It is robust to outliers and does not require feature scaling (although we applied RobustScaler).\n",
        "* It captures non-linear relationships between features and the target (ring_GT) without requiring prior transformation or feature engineering.\n",
        "\n",
        "We expect Random Forest to perform reliably as a first model, providing a strong baseline against other models like Linear Regression"
      ],
      "metadata": {
        "id": "hExMjBmiHi1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To improve the performance of our Random Forest we applied a grid search:\n",
        " * n_estimators: number of trees in the forest,\n",
        " * max_depth: maximum depth of each tree\n",
        " * min_samples_split: minimum samples required to split an internal node.\n",
        "\n",
        "This process helps us identify the best-performing combination of hyperparameters using cross-validation and evaluating with negative RMSE as the scoring metric.\n",
        "\n",
        "In summary, this optimization for our Random Forest helps reduce overfitting and selects a model that better generalizes to unseen data."
      ],
      "metadata": {
        "id": "RXBOU53rIV1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Hyperparameter grid\n",
        "rf_params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# GridSearchCV using RMSE as scoring\n",
        "rf_grid_search = GridSearchCV(\n",
        "    rf_model,\n",
        "    rf_params,\n",
        "    cv=5,\n",
        "    scoring='neg_root_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Fitting\n",
        "rf_grid_search.fit(X_train_pca, y_train)"
      ],
      "metadata": {
        "id": "Ob1cyi-OFcQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**4. Evaluation**"
      ],
      "metadata": {
        "id": "WtFhgSaiJucd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate best model\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "y_rf_pred = best_rf_model.predict(X_test_pca)\n",
        "\n",
        "# Metrics\n",
        "rmse = mean_squared_error(y_test, y_rf_pred)\n",
        "mae = mean_absolute_error(y_test, y_rf_pred)\n",
        "r2 = r2_score(y_test, y_rf_pred)\n",
        "\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(f\"Best Parameters: {rf_grid_search.best_params_}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "metadata": {
        "id": "FJs0lISmJt5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create scatter plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(x=y_test, y=y_rf_pred, alpha=0.7)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Ideal')\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Actual ring_GT\")\n",
        "plt.ylabel(\"Predicted ring_GT\")\n",
        "plt.title(\"Predicted vs Actual ring_GT\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mrFMxoNNL2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Regressor with RobustScaler and PCA performed well on the task of predicting the ring_GT target variable, providing a RMSE of 0.53, MAE of 0.50, and an R² score of 0.69. These results indicate that the model captured a good portion of the variance.\n",
        "\n",
        "On one side, since the features are in millimeters, the RMSE is telling us that, on average, the model's prediction deviate from the actual values by 0,53 mm.\n",
        "\n",
        "On the other side, the R2 score of 0.69 suggests that there is still room for improvement in explaining the variance in the target.\n",
        "\n",
        "We could experiment with other models like SVR which is robust to overfitting and may capture non-linearities more effectively. Other solutions to refine the model would go through testing more values for the hyperparameter, feature engineering or trying different number of components for the PCA."
      ],
      "metadata": {
        "id": "FB2oCxQMc6EL"
      }
    }
  ]
}
